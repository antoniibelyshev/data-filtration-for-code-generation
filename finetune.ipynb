{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from training.training import finetune\n",
    "from datasets import load_dataset, load_from_disk, DatasetDict, Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from filtering import compute_perplexity, filter_dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "disk_path = \"data/train_eval_test_split\"\n",
    "\n",
    "# code_exercises = load_dataset(\"jinaai/code_exercises\")[\"train\"].select(range(25000))\n",
    "# train_test_split = code_exercises.train_test_split(test_size=0.1)\n",
    "# train_eval_split = train_test_split[\"train\"].train_test_split(test_size=1/9)\n",
    "\n",
    "# train_dataset = train_eval_split[\"train\"]\n",
    "# eval_dataset = train_eval_split[\"test\"]\n",
    "# test_dataset = train_test_split[\"test\"]\n",
    "\n",
    "# DatasetDict({\n",
    "#     \"train\": train_dataset,\n",
    "#     \"eval\": eval_dataset,\n",
    "#     \"test\": test_dataset,\n",
    "# }).save_to_disk(disk_path)\n",
    "\n",
    "train_eval_test_split = load_from_disk(disk_path)\n",
    "\n",
    "train_eval_split = DatasetDict({\n",
    "    \"train\": train_eval_test_split[\"train\"],\n",
    "    \"test\": train_eval_test_split[\"eval\"]\n",
    "})\n",
    "\n",
    "test_dataset = train_eval_test_split[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Salesforce/codegen-350M-mono were not used when initializing CodeGenForCausalLM: ['transformer.h.0.attn.causal_mask', 'transformer.h.1.attn.causal_mask', 'transformer.h.10.attn.causal_mask', 'transformer.h.11.attn.causal_mask', 'transformer.h.12.attn.causal_mask', 'transformer.h.13.attn.causal_mask', 'transformer.h.14.attn.causal_mask', 'transformer.h.15.attn.causal_mask', 'transformer.h.16.attn.causal_mask', 'transformer.h.17.attn.causal_mask', 'transformer.h.18.attn.causal_mask', 'transformer.h.19.attn.causal_mask', 'transformer.h.2.attn.causal_mask', 'transformer.h.3.attn.causal_mask', 'transformer.h.4.attn.causal_mask', 'transformer.h.5.attn.causal_mask', 'transformer.h.6.attn.causal_mask', 'transformer.h.7.attn.causal_mask', 'transformer.h.8.attn.causal_mask', 'transformer.h.9.attn.causal_mask']\n",
      "- This IS expected if you are initializing CodeGenForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CodeGenForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Salesforce/codegen-350M-mono\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "if tokenizer.__dict__.get(\"pad_token\") is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2500/2500 [02:33<00:00, 16.28 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw perplexity: 19471.048578253172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "raw_perplexity_dataset = test_dataset.map(compute_perplexity, fn_kwargs={\"model\": model, \"tokenizer\": tokenizer}, batched=True, batch_size=8)\n",
    "raw_perplexity = np.mean(raw_perplexity_dataset[\"perplexity\"])\n",
    "print(\"Raw perplexity:\", raw_perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_kwargs = {\n",
    "    \"per_device_train_batch_size\": 16,\n",
    "    \"warmup_ratio\": 0.1,\n",
    "    \"per_device_eval_batch_size\": 16,\n",
    "    \"gradient_accumulation_steps\": 8,\n",
    "    \"evaluation_strategy\": \"steps\",\n",
    "    \"eval_steps\": 40,\n",
    "    \"save_strategy\": \"steps\",\n",
    "    \"save_steps\": 40,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mantonii-belyshev\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/coder/project/wandb/run-20241203_232424-jebc2z1x</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/antonii-belyshev/dataset_filtration/runs/jebc2z1x' target=\"_blank\">balmy-brook-23</a></strong> to <a href='https://wandb.ai/antonii-belyshev/dataset_filtration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/antonii-belyshev/dataset_filtration' target=\"_blank\">https://wandb.ai/antonii-belyshev/dataset_filtration</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/antonii-belyshev/dataset_filtration/runs/jebc2z1x' target=\"_blank\">https://wandb.ai/antonii-belyshev/dataset_filtration/runs/jebc2z1x</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='314' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [157/157 19:46]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='440' max='1560' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 440/1560 3:12:05 < 8:11:10, 0.04 it/s, Epoch 2/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.143700</td>\n",
       "      <td>0.140454</td>\n",
       "      <td>0.005400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.134400</td>\n",
       "      <td>0.130064</td>\n",
       "      <td>0.005400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.125400</td>\n",
       "      <td>0.127595</td>\n",
       "      <td>0.005400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.109900</td>\n",
       "      <td>0.129537</td>\n",
       "      <td>0.005400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.103600</td>\n",
       "      <td>0.127603</td>\n",
       "      <td>0.005400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.116100</td>\n",
       "      <td>0.126124</td>\n",
       "      <td>0.005400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.106400</td>\n",
       "      <td>0.123922</td>\n",
       "      <td>0.005400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.067700</td>\n",
       "      <td>0.137439</td>\n",
       "      <td>0.005400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.068300</td>\n",
       "      <td>0.136801</td>\n",
       "      <td>0.005400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.062600</td>\n",
       "      <td>0.137668</td>\n",
       "      <td>0.005400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.068800</td>\n",
       "      <td>0.136067</td>\n",
       "      <td>0.005400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/model_preparation_time</td><td>▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>█▅▃▁▂▁▃▂▁▂▂▃</td></tr><tr><td>eval/samples_per_second</td><td>▁▃▆█▇█▆▇█▇▇▆</td></tr><tr><td>eval/steps_per_second</td><td>▁▁▅███▅███▅▅</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇█</td></tr><tr><td>train/grad_norm</td><td>█▇▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>▁▁▁▂▂▃▄▄▅▆▇█████████████▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>train/loss</td><td>██▇▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.13607</td></tr><tr><td>eval/model_preparation_time</td><td>0.0054</td></tr><tr><td>eval/runtime</td><td>146.7159</td></tr><tr><td>eval/samples_per_second</td><td>17.04</td></tr><tr><td>eval/steps_per_second</td><td>1.07</td></tr><tr><td>total_flos</td><td>5.264564753006592e+16</td></tr><tr><td>train/epoch</td><td>2.816</td></tr><tr><td>train/global_step</td><td>440</td></tr><tr><td>train/grad_norm</td><td>1.47672</td></tr><tr><td>train/learning_rate</td><td>4e-05</td></tr><tr><td>train/loss</td><td>0.0688</td></tr><tr><td>train_loss</td><td>0.32559</td></tr><tr><td>train_runtime</td><td>11547.6179</td></tr><tr><td>train_samples_per_second</td><td>17.32</td></tr><tr><td>train_steps_per_second</td><td>0.135</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">balmy-brook-23</strong> at: <a href='https://wandb.ai/antonii-belyshev/dataset_filtration/runs/jebc2z1x' target=\"_blank\">https://wandb.ai/antonii-belyshev/dataset_filtration/runs/jebc2z1x</a><br/> View project at: <a href='https://wandb.ai/antonii-belyshev/dataset_filtration' target=\"_blank\">https://wandb.ai/antonii-belyshev/dataset_filtration</a><br/>Synced 4 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241203_232424-jebc2z1x/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "finetuned_model = finetune(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    train_eval_split,\n",
    "    run_name = \"full_dataset_finetuning\",\n",
    "    **finetune_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2500/2500 [02:33<00:00, 16.28 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test perplexity for model finetuned on the whole train dataset 151.07375120973586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "raw_perplexity_dataset = test_dataset.map(compute_perplexity, fn_kwargs={\"model\": finetuned_model, \"tokenizer\": tokenizer}, batched=True, batch_size=8)\n",
    "finetuned_perplexity = np.mean(raw_perplexity_dataset[\"perplexity\"])\n",
    "print(\"Test perplexity for model finetuned on the whole train dataset\", finetuned_perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_model.save_pretrained(\"finetuned_checkpoints/raw_dataset_finetuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Salesforce/codegen-350M-mono were not used when initializing CodeGenForCausalLM: ['transformer.h.0.attn.causal_mask', 'transformer.h.1.attn.causal_mask', 'transformer.h.10.attn.causal_mask', 'transformer.h.11.attn.causal_mask', 'transformer.h.12.attn.causal_mask', 'transformer.h.13.attn.causal_mask', 'transformer.h.14.attn.causal_mask', 'transformer.h.15.attn.causal_mask', 'transformer.h.16.attn.causal_mask', 'transformer.h.17.attn.causal_mask', 'transformer.h.18.attn.causal_mask', 'transformer.h.19.attn.causal_mask', 'transformer.h.2.attn.causal_mask', 'transformer.h.3.attn.causal_mask', 'transformer.h.4.attn.causal_mask', 'transformer.h.5.attn.causal_mask', 'transformer.h.6.attn.causal_mask', 'transformer.h.7.attn.causal_mask', 'transformer.h.8.attn.causal_mask', 'transformer.h.9.attn.causal_mask']\n",
      "- This IS expected if you are initializing CodeGenForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CodeGenForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Map: 100%|██████████| 20000/20000 [20:27<00:00, 16.29 examples/s]\n",
      "Some weights of the model checkpoint at Salesforce/codegen-350M-mono were not used when initializing CodeGenForCausalLM: ['transformer.h.0.attn.causal_mask', 'transformer.h.1.attn.causal_mask', 'transformer.h.10.attn.causal_mask', 'transformer.h.11.attn.causal_mask', 'transformer.h.12.attn.causal_mask', 'transformer.h.13.attn.causal_mask', 'transformer.h.14.attn.causal_mask', 'transformer.h.15.attn.causal_mask', 'transformer.h.16.attn.causal_mask', 'transformer.h.17.attn.causal_mask', 'transformer.h.18.attn.causal_mask', 'transformer.h.19.attn.causal_mask', 'transformer.h.2.attn.causal_mask', 'transformer.h.3.attn.causal_mask', 'transformer.h.4.attn.causal_mask', 'transformer.h.5.attn.causal_mask', 'transformer.h.6.attn.causal_mask', 'transformer.h.7.attn.causal_mask', 'transformer.h.8.attn.causal_mask', 'transformer.h.9.attn.causal_mask']\n",
      "- This IS expected if you are initializing CodeGenForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CodeGenForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/coder/project/wandb/run-20241204_030240-9jdyzwte</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/antonii-belyshev/dataset_filtration/runs/9jdyzwte' target=\"_blank\">vivid-glitter-24</a></strong> to <a href='https://wandb.ai/antonii-belyshev/dataset_filtration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/antonii-belyshev/dataset_filtration' target=\"_blank\">https://wandb.ai/antonii-belyshev/dataset_filtration</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/antonii-belyshev/dataset_filtration/runs/9jdyzwte' target=\"_blank\">https://wandb.ai/antonii-belyshev/dataset_filtration/runs/9jdyzwte</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='314' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [157/157 19:47]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='240' max='780' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [240/780 1:44:31 < 3:57:08, 0.04 it/s, Epoch 3/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.114000</td>\n",
       "      <td>0.139366</td>\n",
       "      <td>0.002800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.088400</td>\n",
       "      <td>0.135823</td>\n",
       "      <td>0.002800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.081300</td>\n",
       "      <td>0.136481</td>\n",
       "      <td>0.002800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.047700</td>\n",
       "      <td>0.151900</td>\n",
       "      <td>0.002800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.047900</td>\n",
       "      <td>0.152450</td>\n",
       "      <td>0.002800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.029900</td>\n",
       "      <td>0.201103</td>\n",
       "      <td>0.002800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▁▁▁▁▁▁</td></tr><tr><td>eval/model_preparation_time</td><td>▁▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▁▅▄▅▃▄█</td></tr><tr><td>eval/samples_per_second</td><td>█▄▅▄▆▄▁</td></tr><tr><td>eval/steps_per_second</td><td>█▁▁▁▁▁▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇██████</td></tr><tr><td>train/global_step</td><td>▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇██</td></tr><tr><td>train/grad_norm</td><td>███▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>▁▁▂▂▃▃▄▅▅▅███████▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆▆▆▆▆▆</td></tr><tr><td>train/loss</td><td>███▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.2011</td></tr><tr><td>eval/model_preparation_time</td><td>0.0028</td></tr><tr><td>eval/runtime</td><td>146.9268</td></tr><tr><td>eval/samples_per_second</td><td>17.015</td></tr><tr><td>eval/steps_per_second</td><td>1.069</td></tr><tr><td>total_flos</td><td>2.871580774367232e+16</td></tr><tr><td>train/epoch</td><td>3.072</td></tr><tr><td>train/global_step</td><td>240</td></tr><tr><td>train/grad_norm</td><td>1.76752</td></tr><tr><td>train/learning_rate</td><td>4e-05</td></tr><tr><td>train/loss</td><td>0.0299</td></tr><tr><td>train_loss</td><td>0.39561</td></tr><tr><td>train_runtime</td><td>6293.6646</td></tr><tr><td>train_samples_per_second</td><td>15.889</td></tr><tr><td>train_steps_per_second</td><td>0.124</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vivid-glitter-24</strong> at: <a href='https://wandb.ai/antonii-belyshev/dataset_filtration/runs/9jdyzwte' target=\"_blank\">https://wandb.ai/antonii-belyshev/dataset_filtration/runs/9jdyzwte</a><br/> View project at: <a href='https://wandb.ai/antonii-belyshev/dataset_filtration' target=\"_blank\">https://wandb.ai/antonii-belyshev/dataset_filtration</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241204_030240-9jdyzwte/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2500/2500 [02:33<00:00, 16.23 examples/s]\n",
      "Some weights of the model checkpoint at Salesforce/codegen-350M-mono were not used when initializing CodeGenForCausalLM: ['transformer.h.0.attn.causal_mask', 'transformer.h.1.attn.causal_mask', 'transformer.h.10.attn.causal_mask', 'transformer.h.11.attn.causal_mask', 'transformer.h.12.attn.causal_mask', 'transformer.h.13.attn.causal_mask', 'transformer.h.14.attn.causal_mask', 'transformer.h.15.attn.causal_mask', 'transformer.h.16.attn.causal_mask', 'transformer.h.17.attn.causal_mask', 'transformer.h.18.attn.causal_mask', 'transformer.h.19.attn.causal_mask', 'transformer.h.2.attn.causal_mask', 'transformer.h.3.attn.causal_mask', 'transformer.h.4.attn.causal_mask', 'transformer.h.5.attn.causal_mask', 'transformer.h.6.attn.causal_mask', 'transformer.h.7.attn.causal_mask', 'transformer.h.8.attn.causal_mask', 'transformer.h.9.attn.causal_mask']\n",
      "- This IS expected if you are initializing CodeGenForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CodeGenForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/coder/project/wandb/run-20241204_045247-rumtlrjp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/antonii-belyshev/dataset_filtration/runs/rumtlrjp' target=\"_blank\">fast-morning-25</a></strong> to <a href='https://wandb.ai/antonii-belyshev/dataset_filtration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/antonii-belyshev/dataset_filtration' target=\"_blank\">https://wandb.ai/antonii-belyshev/dataset_filtration</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/antonii-belyshev/dataset_filtration/runs/rumtlrjp' target=\"_blank\">https://wandb.ai/antonii-belyshev/dataset_filtration/runs/rumtlrjp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='314' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [157/157 19:47]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='280' max='780' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [280/780 2:01:54 < 3:39:16, 0.04 it/s, Epoch 3/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.131400</td>\n",
       "      <td>0.137223</td>\n",
       "      <td>0.002800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.079600</td>\n",
       "      <td>0.133750</td>\n",
       "      <td>0.002800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.081400</td>\n",
       "      <td>0.132571</td>\n",
       "      <td>0.002800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.065200</td>\n",
       "      <td>0.150297</td>\n",
       "      <td>0.002800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.062400</td>\n",
       "      <td>0.148219</td>\n",
       "      <td>0.002800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.032700</td>\n",
       "      <td>0.195775</td>\n",
       "      <td>0.002800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.035900</td>\n",
       "      <td>0.179650</td>\n",
       "      <td>0.002800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▁▁▁▁▁▁▁</td></tr><tr><td>eval/model_preparation_time</td><td>▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▂▂▁▅▃▇▂█</td></tr><tr><td>eval/samples_per_second</td><td>▇▆█▄▆▂▇▁</td></tr><tr><td>eval/steps_per_second</td><td>▁▁▁▁▁▁▁▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇█████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▇▇▇▇▇▇▇▇███████</td></tr><tr><td>train/grad_norm</td><td>█▆▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>▁▄▄▅▆▇▇▇▇██████████▇▇▇▇▇▇▇▇▇▇▇▇▇▆▆▆▆▆▆▆▆</td></tr><tr><td>train/loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.17965</td></tr><tr><td>eval/model_preparation_time</td><td>0.0028</td></tr><tr><td>eval/runtime</td><td>146.9167</td></tr><tr><td>eval/samples_per_second</td><td>17.016</td></tr><tr><td>eval/steps_per_second</td><td>1.069</td></tr><tr><td>total_flos</td><td>3.350177570095104e+16</td></tr><tr><td>train/epoch</td><td>3.584</td></tr><tr><td>train/global_step</td><td>280</td></tr><tr><td>train/grad_norm</td><td>1.60805</td></tr><tr><td>train/learning_rate</td><td>4e-05</td></tr><tr><td>train/loss</td><td>0.0359</td></tr><tr><td>train_loss</td><td>0.34775</td></tr><tr><td>train_runtime</td><td>7337.2692</td></tr><tr><td>train_samples_per_second</td><td>13.629</td></tr><tr><td>train_steps_per_second</td><td>0.106</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fast-morning-25</strong> at: <a href='https://wandb.ai/antonii-belyshev/dataset_filtration/runs/rumtlrjp' target=\"_blank\">https://wandb.ai/antonii-belyshev/dataset_filtration/runs/rumtlrjp</a><br/> View project at: <a href='https://wandb.ai/antonii-belyshev/dataset_filtration' target=\"_blank\">https://wandb.ai/antonii-belyshev/dataset_filtration</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241204_045247-rumtlrjp/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2500/2500 [02:34<00:00, 16.21 examples/s]\n",
      "Some weights of the model checkpoint at Salesforce/codegen-350M-mono were not used when initializing CodeGenForCausalLM: ['transformer.h.0.attn.causal_mask', 'transformer.h.1.attn.causal_mask', 'transformer.h.10.attn.causal_mask', 'transformer.h.11.attn.causal_mask', 'transformer.h.12.attn.causal_mask', 'transformer.h.13.attn.causal_mask', 'transformer.h.14.attn.causal_mask', 'transformer.h.15.attn.causal_mask', 'transformer.h.16.attn.causal_mask', 'transformer.h.17.attn.causal_mask', 'transformer.h.18.attn.causal_mask', 'transformer.h.19.attn.causal_mask', 'transformer.h.2.attn.causal_mask', 'transformer.h.3.attn.causal_mask', 'transformer.h.4.attn.causal_mask', 'transformer.h.5.attn.causal_mask', 'transformer.h.6.attn.causal_mask', 'transformer.h.7.attn.causal_mask', 'transformer.h.8.attn.causal_mask', 'transformer.h.9.attn.causal_mask']\n",
      "- This IS expected if you are initializing CodeGenForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CodeGenForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/coder/project/wandb/run-20241204_070018-mo6cgwin</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/antonii-belyshev/dataset_filtration/runs/mo6cgwin' target=\"_blank\">golden-oath-26</a></strong> to <a href='https://wandb.ai/antonii-belyshev/dataset_filtration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/antonii-belyshev/dataset_filtration' target=\"_blank\">https://wandb.ai/antonii-belyshev/dataset_filtration</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/antonii-belyshev/dataset_filtration/runs/mo6cgwin' target=\"_blank\">https://wandb.ai/antonii-belyshev/dataset_filtration/runs/mo6cgwin</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='314' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [157/157 19:47]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='280' max='780' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [280/780 2:01:50 < 3:39:08, 0.04 it/s, Epoch 3/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.140100</td>\n",
       "      <td>0.136030</td>\n",
       "      <td>0.003000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.115000</td>\n",
       "      <td>0.130943</td>\n",
       "      <td>0.003000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.131300</td>\n",
       "      <td>0.129906</td>\n",
       "      <td>0.003000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.081300</td>\n",
       "      <td>0.148275</td>\n",
       "      <td>0.003000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.074100</td>\n",
       "      <td>0.143403</td>\n",
       "      <td>0.003000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.055300</td>\n",
       "      <td>0.186252</td>\n",
       "      <td>0.003000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.048600</td>\n",
       "      <td>0.169341</td>\n",
       "      <td>0.003000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▁▁▁▁▁▁▁</td></tr><tr><td>eval/model_preparation_time</td><td>▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>█▄▃▆▁▅▃▂</td></tr><tr><td>eval/samples_per_second</td><td>▁▅▆▃█▄▆▇</td></tr><tr><td>eval/steps_per_second</td><td>▁██▁████</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇██</td></tr><tr><td>train/grad_norm</td><td>███▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>▁▂▂▂▃▄▅▅▆▇█████▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆▆▆▆▆▆▆▆▆</td></tr><tr><td>train/loss</td><td>█▇▆▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.16934</td></tr><tr><td>eval/model_preparation_time</td><td>0.003</td></tr><tr><td>eval/runtime</td><td>146.8737</td></tr><tr><td>eval/samples_per_second</td><td>17.021</td></tr><tr><td>eval/steps_per_second</td><td>1.069</td></tr><tr><td>total_flos</td><td>3.350177570095104e+16</td></tr><tr><td>train/epoch</td><td>3.584</td></tr><tr><td>train/global_step</td><td>280</td></tr><tr><td>train/grad_norm</td><td>1.80093</td></tr><tr><td>train/learning_rate</td><td>4e-05</td></tr><tr><td>train/loss</td><td>0.0486</td></tr><tr><td>train_loss</td><td>0.35248</td></tr><tr><td>train_runtime</td><td>7332.7701</td></tr><tr><td>train_samples_per_second</td><td>13.637</td></tr><tr><td>train_steps_per_second</td><td>0.106</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">golden-oath-26</strong> at: <a href='https://wandb.ai/antonii-belyshev/dataset_filtration/runs/mo6cgwin' target=\"_blank\">https://wandb.ai/antonii-belyshev/dataset_filtration/runs/mo6cgwin</a><br/> View project at: <a href='https://wandb.ai/antonii-belyshev/dataset_filtration' target=\"_blank\">https://wandb.ai/antonii-belyshev/dataset_filtration</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241204_070018-mo6cgwin/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2500/2500 [02:33<00:00, 16.23 examples/s]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name).cuda()\n",
    "train_dataset = train_eval_split[\"train\"].map(compute_perplexity, fn_kwargs={\"model\": model, \"tokenizer\": tokenizer}, batched=True, batch_size=8)\n",
    "eval_dataset = train_eval_split[\"test\"]\n",
    "\n",
    "perplexities = []\n",
    "for part in [\"low\", \"mid\", \"high\"]:\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name).cuda()\n",
    "    dataset = DatasetDict({\n",
    "        \"train\": filter_dataset(train_dataset, \"perplexity\", 0.5, part),\n",
    "        \"test\": eval_dataset\n",
    "    })\n",
    "\n",
    "    name = f\"{part}_perplexity_dataset_finetuning\"\n",
    "    finetuned_model = finetune(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        dataset,\n",
    "        run_name = name,\n",
    "        **finetune_kwargs\n",
    "    )\n",
    "\n",
    "    finetuned_perplexity_dataset = test_dataset.map(compute_perplexity, fn_kwargs={\"model\": finetuned_model, \"tokenizer\": tokenizer}, batched=True, batch_size=8)\n",
    "    perplexities.append(np.mean(finetuned_perplexity_dataset[\"perplexity\"]))\n",
    "\n",
    "    finetuned_model.save_pretrained(f\"finetuned_checkpoints/{name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test perplexity for model finetuned on low perplexity samples: 144.29739919719697\n",
      "Test perplexity for model finetuned on middle perplexity samples: 166.25313509335518\n",
      "Test perplexity for model finetuned on high perplexity samples: 209.86736042041778\n"
     ]
    }
   ],
   "source": [
    "for ppl, part in zip(perplexities, [\"low\", \"middle\", \"high\"]):\n",
    "    print(f\"Test perplexity for model finetuned on {part} perplexity samples:\", ppl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def get_absolute_values(numbers):\n",
      "    \"\"\"\n",
      "    Given a list of numbers, calculate the absolute value for each number and return a set of absolute values.\n",
      "\n",
      "    Inputs:\n",
      "    - numbers: a list of numbers\n",
      "\n",
      "    Returns:\n",
      "    - absolute_set: a set containing the absolute values of the input numbers\n",
      "    \"\"\"\n",
      "    absolute_set = set()\n",
      "    for num in numbers:\n",
      "        absolute_value = abs(num)\n",
      "        absolute_set.add(absolute_value)\n",
      "    return absolute_set\n",
      "\n",
      "\n",
      "<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(train_eval_split[\"test\"][0][\"problem\"], return_tensors=\"pt\").to(finetuned_model.device)\n",
    "\n",
    "generated_code = tokenizer.decode(finetuned_model.generate(input_ids, max_new_tokens=200)[0])\n",
    "print(generated_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
