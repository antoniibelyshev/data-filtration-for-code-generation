{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from training.training import finetune\n",
    "from datasets import load_dataset, load_from_disk, DatasetDict, Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from filtering import compute_perplexity, filter_dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "disk_path = \"data/train_eval_test_split\"\n",
    "\n",
    "# code_exercises = load_dataset(\"jinaai/code_exercises\")[\"train\"].select(range(25000))\n",
    "# train_test_split = code_exercises.train_test_split(test_size=0.1)\n",
    "# train_eval_split = train_test_split[\"train\"].train_test_split(test_size=1/9)\n",
    "\n",
    "# train_dataset = train_eval_split[\"train\"]\n",
    "# eval_dataset = train_eval_split[\"test\"]\n",
    "# test_dataset = train_test_split[\"test\"]\n",
    "\n",
    "# DatasetDict({\n",
    "#     \"train\": train_dataset,\n",
    "#     \"eval\": eval_dataset,\n",
    "#     \"test\": test_dataset,\n",
    "# }).save_to_disk(disk_path)\n",
    "\n",
    "train_eval_test_split = load_from_disk(disk_path)\n",
    "\n",
    "train_eval_split = DatasetDict({\n",
    "    \"train\": train_eval_test_split[\"train\"],\n",
    "    \"test\": train_eval_test_split[\"eval\"]\n",
    "})\n",
    "\n",
    "test_dataset = train_eval_test_split[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Salesforce/codegen-350M-mono were not used when initializing CodeGenForCausalLM: ['transformer.h.0.attn.causal_mask', 'transformer.h.1.attn.causal_mask', 'transformer.h.10.attn.causal_mask', 'transformer.h.11.attn.causal_mask', 'transformer.h.12.attn.causal_mask', 'transformer.h.13.attn.causal_mask', 'transformer.h.14.attn.causal_mask', 'transformer.h.15.attn.causal_mask', 'transformer.h.16.attn.causal_mask', 'transformer.h.17.attn.causal_mask', 'transformer.h.18.attn.causal_mask', 'transformer.h.19.attn.causal_mask', 'transformer.h.2.attn.causal_mask', 'transformer.h.3.attn.causal_mask', 'transformer.h.4.attn.causal_mask', 'transformer.h.5.attn.causal_mask', 'transformer.h.6.attn.causal_mask', 'transformer.h.7.attn.causal_mask', 'transformer.h.8.attn.causal_mask', 'transformer.h.9.attn.causal_mask']\n",
      "- This IS expected if you are initializing CodeGenForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CodeGenForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Salesforce/codegen-350M-mono\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "if tokenizer.__dict__.get(\"pad_token\") is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2500/2500 [02:33<00:00, 16.31 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw perplexity: 19471.048578253172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "raw_perplexity_dataset = test_dataset.map(compute_perplexity, fn_kwargs={\"model\": model, \"tokenizer\": tokenizer}, batched=True, batch_size=8)\n",
    "raw_perplexity = np.mean(raw_perplexity_dataset[\"perplexity\"])\n",
    "print(\"Raw perplexity:\", raw_perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mantonii-belyshev\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/coder/project/wandb/run-20241127_112227-tooxvli8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/antonii-belyshev/dataset_filtration/runs/tooxvli8' target=\"_blank\">golden-gorge-18</a></strong> to <a href='https://wandb.ai/antonii-belyshev/dataset_filtration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/antonii-belyshev/dataset_filtration' target=\"_blank\">https://wandb.ai/antonii-belyshev/dataset_filtration</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/antonii-belyshev/dataset_filtration/runs/tooxvli8' target=\"_blank\">https://wandb.ai/antonii-belyshev/dataset_filtration/runs/tooxvli8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='234' max='780' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [234/780 2:59:55 < 7:03:26, 0.02 it/s, Epoch 2/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.132900</td>\n",
       "      <td>0.126102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.100400</td>\n",
       "      <td>0.121797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.073900</td>\n",
       "      <td>0.132517</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▄▁█</td></tr><tr><td>eval/runtime</td><td>▁█▃</td></tr><tr><td>eval/samples_per_second</td><td>█▁▆</td></tr><tr><td>eval/steps_per_second</td><td>█▁█</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇█</td></tr><tr><td>train/grad_norm</td><td>██▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>▁▁▂▂▄▄▄▅▅▅▆▆▇███████████▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆</td></tr><tr><td>train/loss</td><td>██▆▄▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.13252</td></tr><tr><td>eval/runtime</td><td>145.6162</td></tr><tr><td>eval/samples_per_second</td><td>17.168</td></tr><tr><td>eval/steps_per_second</td><td>1.078</td></tr><tr><td>total_flos</td><td>5.608556199936e+16</td></tr><tr><td>train/epoch</td><td>2.9952</td></tr><tr><td>train/global_step</td><td>234</td></tr><tr><td>train/grad_norm</td><td>2.34258</td></tr><tr><td>train/learning_rate</td><td>4e-05</td></tr><tr><td>train/loss</td><td>0.0739</td></tr><tr><td>train_loss</td><td>0.41199</td></tr><tr><td>train_runtime</td><td>10839.8448</td></tr><tr><td>train_samples_per_second</td><td>18.45</td></tr><tr><td>train_steps_per_second</td><td>0.072</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">golden-gorge-18</strong> at: <a href='https://wandb.ai/antonii-belyshev/dataset_filtration/runs/tooxvli8' target=\"_blank\">https://wandb.ai/antonii-belyshev/dataset_filtration/runs/tooxvli8</a><br/> View project at: <a href='https://wandb.ai/antonii-belyshev/dataset_filtration' target=\"_blank\">https://wandb.ai/antonii-belyshev/dataset_filtration</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241127_112227-tooxvli8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "finetuned_model = finetune(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    train_eval_split,\n",
    "    run_name = \"full_dataset_finetuning\",\n",
    "    per_device_train_batch_size = 16,\n",
    "    warmup_ratio = 0.1,\n",
    "    per_device_eval_batch_size = 16,\n",
    "    gradient_accumulation_steps = 16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2500/2500 [02:32<00:00, 16.37 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test perplexity for model finetuned on the whole train dataset 207.0812493841648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "raw_perplexity_dataset = test_dataset.map(compute_perplexity, fn_kwargs={\"model\": finetuned_model, \"tokenizer\": tokenizer}, batched=True, batch_size=8)\n",
    "finetuned_perplexity = np.mean(raw_perplexity_dataset[\"perplexity\"])\n",
    "print(\"Test perplexity for model finetuned on the whole train dataset\", finetuned_perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_model.save_pretrained(\"finetuned_checkpoints/raw_dataset_finetuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Salesforce/codegen-350M-mono were not used when initializing CodeGenForCausalLM: ['transformer.h.0.attn.causal_mask', 'transformer.h.1.attn.causal_mask', 'transformer.h.10.attn.causal_mask', 'transformer.h.11.attn.causal_mask', 'transformer.h.12.attn.causal_mask', 'transformer.h.13.attn.causal_mask', 'transformer.h.14.attn.causal_mask', 'transformer.h.15.attn.causal_mask', 'transformer.h.16.attn.causal_mask', 'transformer.h.17.attn.causal_mask', 'transformer.h.18.attn.causal_mask', 'transformer.h.19.attn.causal_mask', 'transformer.h.2.attn.causal_mask', 'transformer.h.3.attn.causal_mask', 'transformer.h.4.attn.causal_mask', 'transformer.h.5.attn.causal_mask', 'transformer.h.6.attn.causal_mask', 'transformer.h.7.attn.causal_mask', 'transformer.h.8.attn.causal_mask', 'transformer.h.9.attn.causal_mask']\n",
      "- This IS expected if you are initializing CodeGenForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CodeGenForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at Salesforce/codegen-350M-mono were not used when initializing CodeGenForCausalLM: ['transformer.h.0.attn.causal_mask', 'transformer.h.1.attn.causal_mask', 'transformer.h.10.attn.causal_mask', 'transformer.h.11.attn.causal_mask', 'transformer.h.12.attn.causal_mask', 'transformer.h.13.attn.causal_mask', 'transformer.h.14.attn.causal_mask', 'transformer.h.15.attn.causal_mask', 'transformer.h.16.attn.causal_mask', 'transformer.h.17.attn.causal_mask', 'transformer.h.18.attn.causal_mask', 'transformer.h.19.attn.causal_mask', 'transformer.h.2.attn.causal_mask', 'transformer.h.3.attn.causal_mask', 'transformer.h.4.attn.causal_mask', 'transformer.h.5.attn.causal_mask', 'transformer.h.6.attn.causal_mask', 'transformer.h.7.attn.causal_mask', 'transformer.h.8.attn.causal_mask', 'transformer.h.9.attn.causal_mask']\n",
      "- This IS expected if you are initializing CodeGenForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CodeGenForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/coder/project/wandb/run-20241119_175304-8bfk3h2m</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/antonii-belyshev/dataset_filtration/runs/8bfk3h2m' target=\"_blank\">breezy-blaze-14</a></strong> to <a href='https://wandb.ai/antonii-belyshev/dataset_filtration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/antonii-belyshev/dataset_filtration' target=\"_blank\">https://wandb.ai/antonii-belyshev/dataset_filtration</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/antonii-belyshev/dataset_filtration/runs/8bfk3h2m' target=\"_blank\">https://wandb.ai/antonii-belyshev/dataset_filtration/runs/8bfk3h2m</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1875' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1875/6250 41:23 < 1:36:40, 0.75 it/s, Epoch 3/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.100400</td>\n",
       "      <td>0.146450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.063300</td>\n",
       "      <td>0.146437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.163443</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁▁█</td></tr><tr><td>eval/runtime</td><td>▃▁█</td></tr><tr><td>eval/samples_per_second</td><td>▆█▁</td></tr><tr><td>eval/steps_per_second</td><td>██▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇█████</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/grad_norm</td><td>██▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>▁▂▂▂▃▃▃▃▄▄▆▇▇██████████▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>train/loss</td><td>██▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.16344</td></tr><tr><td>eval/runtime</td><td>63.9521</td></tr><tr><td>eval/samples_per_second</td><td>39.092</td></tr><tr><td>eval/steps_per_second</td><td>2.455</td></tr><tr><td>total_flos</td><td>2.804278099968e+16</td></tr><tr><td>train/epoch</td><td>3</td></tr><tr><td>train/global_step</td><td>1875</td></tr><tr><td>train/grad_norm</td><td>0.35963</td></tr><tr><td>train/learning_rate</td><td>4e-05</td></tr><tr><td>train/loss</td><td>0.05</td></tr><tr><td>train_loss</td><td>0.18815</td></tr><tr><td>train_runtime</td><td>2484.5179</td></tr><tr><td>train_samples_per_second</td><td>40.249</td></tr><tr><td>train_steps_per_second</td><td>2.516</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">breezy-blaze-14</strong> at: <a href='https://wandb.ai/antonii-belyshev/dataset_filtration/runs/8bfk3h2m' target=\"_blank\">https://wandb.ai/antonii-belyshev/dataset_filtration/runs/8bfk3h2m</a><br/> View project at: <a href='https://wandb.ai/antonii-belyshev/dataset_filtration' target=\"_blank\">https://wandb.ai/antonii-belyshev/dataset_filtration</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241119_175304-8bfk3h2m/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2500/2500 [01:07<00:00, 36.88 examples/s]\n",
      "Some weights of the model checkpoint at Salesforce/codegen-350M-mono were not used when initializing CodeGenForCausalLM: ['transformer.h.0.attn.causal_mask', 'transformer.h.1.attn.causal_mask', 'transformer.h.10.attn.causal_mask', 'transformer.h.11.attn.causal_mask', 'transformer.h.12.attn.causal_mask', 'transformer.h.13.attn.causal_mask', 'transformer.h.14.attn.causal_mask', 'transformer.h.15.attn.causal_mask', 'transformer.h.16.attn.causal_mask', 'transformer.h.17.attn.causal_mask', 'transformer.h.18.attn.causal_mask', 'transformer.h.19.attn.causal_mask', 'transformer.h.2.attn.causal_mask', 'transformer.h.3.attn.causal_mask', 'transformer.h.4.attn.causal_mask', 'transformer.h.5.attn.causal_mask', 'transformer.h.6.attn.causal_mask', 'transformer.h.7.attn.causal_mask', 'transformer.h.8.attn.causal_mask', 'transformer.h.9.attn.causal_mask']\n",
      "- This IS expected if you are initializing CodeGenForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CodeGenForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/coder/project/wandb/run-20241119_183547-e7xczdkp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/antonii-belyshev/dataset_filtration/runs/e7xczdkp' target=\"_blank\">daily-mountain-15</a></strong> to <a href='https://wandb.ai/antonii-belyshev/dataset_filtration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/antonii-belyshev/dataset_filtration' target=\"_blank\">https://wandb.ai/antonii-belyshev/dataset_filtration</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/antonii-belyshev/dataset_filtration/runs/e7xczdkp' target=\"_blank\">https://wandb.ai/antonii-belyshev/dataset_filtration/runs/e7xczdkp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1250' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1250/6250 27:33 < 1:50:25, 0.75 it/s, Epoch 2/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.079700</td>\n",
       "      <td>0.142808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.135500</td>\n",
       "      <td>0.142875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁█</td></tr><tr><td>eval/runtime</td><td>▁█</td></tr><tr><td>eval/samples_per_second</td><td>█▁</td></tr><tr><td>eval/steps_per_second</td><td>▁▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇██</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/grad_norm</td><td>▆▇▇██▅▅▆▅▆▅▄▅▄▃▃▃▃▅▃▄▄▃▆▂▃▃▄▃▃▃▃▂▁▄▃▁▄▃▄</td></tr><tr><td>train/learning_rate</td><td>▁▂▂▂▃▄▄▄▅▅▅▆▆▆▆▆▇▇▇█████████▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>train/loss</td><td>▇▄▅█▇▄▄▂▅▇█▅█▆▁▃▃▃▂▃▃▅▂▂▇▅▂▃▄▅▄▂▃▅▁▄▃▂▅▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.14287</td></tr><tr><td>eval/runtime</td><td>63.9564</td></tr><tr><td>eval/samples_per_second</td><td>39.089</td></tr><tr><td>eval/steps_per_second</td><td>2.455</td></tr><tr><td>total_flos</td><td>1.869518733312e+16</td></tr><tr><td>train/epoch</td><td>2</td></tr><tr><td>train/global_step</td><td>1250</td></tr><tr><td>train/grad_norm</td><td>0.49002</td></tr><tr><td>train/learning_rate</td><td>4e-05</td></tr><tr><td>train/loss</td><td>0.1355</td></tr><tr><td>train_loss</td><td>0.26477</td></tr><tr><td>train_runtime</td><td>1654.9648</td></tr><tr><td>train_samples_per_second</td><td>60.424</td></tr><tr><td>train_steps_per_second</td><td>3.777</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">daily-mountain-15</strong> at: <a href='https://wandb.ai/antonii-belyshev/dataset_filtration/runs/e7xczdkp' target=\"_blank\">https://wandb.ai/antonii-belyshev/dataset_filtration/runs/e7xczdkp</a><br/> View project at: <a href='https://wandb.ai/antonii-belyshev/dataset_filtration' target=\"_blank\">https://wandb.ai/antonii-belyshev/dataset_filtration</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241119_183547-e7xczdkp/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2500/2500 [01:07<00:00, 36.88 examples/s]\n",
      "Some weights of the model checkpoint at Salesforce/codegen-350M-mono were not used when initializing CodeGenForCausalLM: ['transformer.h.0.attn.causal_mask', 'transformer.h.1.attn.causal_mask', 'transformer.h.10.attn.causal_mask', 'transformer.h.11.attn.causal_mask', 'transformer.h.12.attn.causal_mask', 'transformer.h.13.attn.causal_mask', 'transformer.h.14.attn.causal_mask', 'transformer.h.15.attn.causal_mask', 'transformer.h.16.attn.causal_mask', 'transformer.h.17.attn.causal_mask', 'transformer.h.18.attn.causal_mask', 'transformer.h.19.attn.causal_mask', 'transformer.h.2.attn.causal_mask', 'transformer.h.3.attn.causal_mask', 'transformer.h.4.attn.causal_mask', 'transformer.h.5.attn.causal_mask', 'transformer.h.6.attn.causal_mask', 'transformer.h.7.attn.causal_mask', 'transformer.h.8.attn.causal_mask', 'transformer.h.9.attn.causal_mask']\n",
      "- This IS expected if you are initializing CodeGenForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CodeGenForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/coder/project/wandb/run-20241119_190440-h6rr2keh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/antonii-belyshev/dataset_filtration/runs/h6rr2keh' target=\"_blank\">bumbling-river-16</a></strong> to <a href='https://wandb.ai/antonii-belyshev/dataset_filtration' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/antonii-belyshev/dataset_filtration' target=\"_blank\">https://wandb.ai/antonii-belyshev/dataset_filtration</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/antonii-belyshev/dataset_filtration/runs/h6rr2keh' target=\"_blank\">https://wandb.ai/antonii-belyshev/dataset_filtration/runs/h6rr2keh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/coder/.local/lib/python310-conda-cuda-torch21/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1875' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1875/6250 41:25 < 1:36:46, 0.75 it/s, Epoch 3/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.186900</td>\n",
       "      <td>0.138583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.115900</td>\n",
       "      <td>0.137677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.059700</td>\n",
       "      <td>0.150426</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>▁▁█</td></tr><tr><td>eval/runtime</td><td>▁█▇</td></tr><tr><td>eval/samples_per_second</td><td>█▁▂</td></tr><tr><td>eval/steps_per_second</td><td>█▁▃</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▃▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇██</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇█████</td></tr><tr><td>train/grad_norm</td><td>█▇▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>▁▁▁▁▂▃▄▅▅▅▆▇▇████████▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆▆</td></tr><tr><td>train/loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.15043</td></tr><tr><td>eval/runtime</td><td>63.9749</td></tr><tr><td>eval/samples_per_second</td><td>39.078</td></tr><tr><td>eval/steps_per_second</td><td>2.454</td></tr><tr><td>total_flos</td><td>2.804278099968e+16</td></tr><tr><td>train/epoch</td><td>3</td></tr><tr><td>train/global_step</td><td>1875</td></tr><tr><td>train/grad_norm</td><td>0.37278</td></tr><tr><td>train/learning_rate</td><td>4e-05</td></tr><tr><td>train/loss</td><td>0.0597</td></tr><tr><td>train_loss</td><td>0.21451</td></tr><tr><td>train_runtime</td><td>2487.0841</td></tr><tr><td>train_samples_per_second</td><td>40.208</td></tr><tr><td>train_steps_per_second</td><td>2.513</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">bumbling-river-16</strong> at: <a href='https://wandb.ai/antonii-belyshev/dataset_filtration/runs/h6rr2keh' target=\"_blank\">https://wandb.ai/antonii-belyshev/dataset_filtration/runs/h6rr2keh</a><br/> View project at: <a href='https://wandb.ai/antonii-belyshev/dataset_filtration' target=\"_blank\">https://wandb.ai/antonii-belyshev/dataset_filtration</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241119_190440-h6rr2keh/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2500/2500 [01:07<00:00, 36.89 examples/s]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name).cuda()\n",
    "train_dataset = train_eval_split[\"train\"].map(compute_perplexity, fn_kwargs={\"model\": model, \"tokenizer\": tokenizer}, batched=True, batch_size=8)\n",
    "eval_dataset = train_eval_split[\"test\"]\n",
    "\n",
    "perplexities = []\n",
    "for part in [\"low\", \"mid\", \"high\"]:\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name).cuda()\n",
    "    dataset = DatasetDict({\n",
    "        \"train\": filter_dataset(train_dataset, \"perplexity\", 0.5, part),\n",
    "        \"test\": eval_dataset\n",
    "    })\n",
    "    finetuned_model = finetune(model, tokenizer, dataset, per_device_train_batch_size=16, warmup_ratio=0.1, per_device_eval_batch_size=16)\n",
    "\n",
    "    finetuned_perplexity_dataset = test_dataset.map(compute_perplexity, fn_kwargs={\"model\": finetuned_model, \"tokenizer\": tokenizer}, batched=True, batch_size=8)\n",
    "    perplexities.append(np.mean(finetuned_perplexity_dataset[\"perplexity\"]))\n",
    "\n",
    "    finetuned_model.save_pretrained(f\"finetuned_checkpoints/{part}_perplexity_dataset_finetuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test perplexity for model finetuned on low perplexity samples: 81.54079272732734\n",
      "Test perplexity for model finetuned on middle perplexity samples: 62.40870286650657\n",
      "Test perplexity for model finetuned on high perplexity samples: 100.16834101648331\n"
     ]
    }
   ],
   "source": [
    "for ppl, part in zip(perplexities, [\"low\", \"middle\", \"high\"]):\n",
    "    print(f\"Test perplexity for model finetuned on {part} perplexity samples:\", ppl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def get_absolute_values(numbers):\n",
      "    \"\"\"\n",
      "    Given a list of numbers, calculate the absolute value for each number and return a set of absolute values.\n",
      "\n",
      "    Inputs:\n",
      "    - numbers: a list of numbers\n",
      "\n",
      "    Returns:\n",
      "    - absolute_set: a set containing the absolute values of the input numbers\n",
      "    \"\"\"\n",
      "    absolute_set = set()\n",
      "    for num in numbers:\n",
      "        absolute_value = abs(num)\n",
      "        absolute_set.add(absolute_value)\n",
      "    return absolute_set\n",
      "\n",
      "\n",
      "<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(train_eval_split[\"test\"][0][\"problem\"], return_tensors=\"pt\").to(finetuned_model.device)\n",
    "\n",
    "generated_code = tokenizer.decode(finetuned_model.generate(input_ids, max_new_tokens=200)[0])\n",
    "print(generated_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
